{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24778d2",
   "metadata": {},
   "source": [
    "# Spark API Mini Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402dcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ffd9c",
   "metadata": {},
   "source": [
    "#### 1. Spark Dataframe Basics\n",
    "\n",
    "i. Use the starter code below to create a pandas dataframe (just run the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dcad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"n\": np.random.randn(20),\n",
    "    \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "    \"abool\": np.random.choice([True, False], 20),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a047d0",
   "metadata": {},
   "source": [
    "ii. Convert the pandas dataframe to a spark dataframe. From this point forward, do all of your work with the spark dataframe, not the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ebe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e739e9f",
   "metadata": {},
   "source": [
    "iii. Show the first 3 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07581df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228ecf8",
   "metadata": {},
   "source": [
    "iv. Show the first 7 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea965",
   "metadata": {},
   "source": [
    "v. View a summary of the data using `.describe()`.\n",
    "> Note that `.describe` returns another dataframe, so we still have to do `.show()` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0dd8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+\n",
      "|summary|                 n|group|\n",
      "+-------+------------------+-----+\n",
      "|  count|                20|   20|\n",
      "|   mean|0.3664026449885217| null|\n",
      "| stddev|0.8905322898155363| null|\n",
      "|    min|-1.261605945319069|    x|\n",
      "|    max|2.1503829673811126|    z|\n",
      "+-------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f600502",
   "metadata": {},
   "source": [
    "vi. Use `.select()` to create a new dataframe with just the `n` and `abool` columns. View the first 5 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b653a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(spark_df.n, spark_df.abool).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90b625",
   "metadata": {},
   "source": [
    "vii. Use `.select()` to create a new dataframe with just the `group` and `abool` columns. View the first 5 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07dfb57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(spark_df.group, spark_df.abool).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9d0c2",
   "metadata": {},
   "source": [
    "viii. Use `.select()` to create a new dataframe with the `group` column and the `abool` column renamed to `a_boolean_value`. Show the first 3 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fc8eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|a_boolean_value|\n",
      "+-----+---------------+\n",
      "|    z|          false|\n",
      "|    x|          false|\n",
      "|    z|          false|\n",
      "+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_spark_df = spark_df.select(spark_df.group, spark_df.abool.alias('a_boolean_value'))\n",
    "new_spark_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b1fe7",
   "metadata": {},
   "source": [
    "ix. Use `.select()` to create a new dataframe with the `group` column and the `n` column renamed to `a_numeric_value`. Show the first 6 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975b66f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|     a_numeric_value|\n",
      "+-----+--------------------+\n",
      "|    z|  -0.712390662050588|\n",
      "|    x|   0.753766378659703|\n",
      "|    z|-0.04450307833805...|\n",
      "|    y| 0.45181233874578974|\n",
      "|    z|  1.3451017084510097|\n",
      "|    y|  0.5323378882945463|\n",
      "+-----+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_new_spark_df = spark_df.select(spark_df.group, spark_df.n.alias('a_numeric_value'))\n",
    "new_new_spark_df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758611f",
   "metadata": {},
   "source": [
    "#### 2. Column Manipulation\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe. Store the spark dataframe in a variable named `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54c36b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae4e84",
   "metadata": {},
   "source": [
    "ii. Use `select()` to add 4 to the `n` column. Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eec5025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             (n + 1)|\n",
      "+--------------------+\n",
      "|   0.287609337949412|\n",
      "|  1.7537663786597029|\n",
      "|  0.9554969216619466|\n",
      "|  1.4518123387457897|\n",
      "|  2.3451017084510095|\n",
      "|  1.5323378882945464|\n",
      "|  2.3501878997225267|\n",
      "|  1.8612113741693206|\n",
      "|   2.478685737435897|\n",
      "|-0.04537713053853...|\n",
      "| 0.21101097504845112|\n",
      "| -0.2616059453190691|\n",
      "|  1.5628467852810313|\n",
      "|  0.7566737481144374|\n",
      "|  1.9137407048596775|\n",
      "|   1.317350922736336|\n",
      "|  1.1273032802069807|\n",
      "|  3.1503829673811126|\n",
      "|  1.6062886568962988|\n",
      "|  0.9732283500135592|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(spark_df.n + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5923f8",
   "metadata": {},
   "source": [
    "iii. Subtract 5 from the `n` column and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3891f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|            (n - 5)|\n",
      "+-------------------+\n",
      "| -5.712390662050588|\n",
      "| -4.246233621340297|\n",
      "| -5.044503078338053|\n",
      "|  -4.54818766125421|\n",
      "|-3.6548982915489905|\n",
      "| -4.467662111705454|\n",
      "|-3.6498121002774733|\n",
      "|  -4.13878862583068|\n",
      "| -3.521314262564103|\n",
      "| -6.045377130538534|\n",
      "| -5.788989024951549|\n",
      "| -6.261605945319069|\n",
      "| -4.437153214718968|\n",
      "| -5.243326251885563|\n",
      "| -4.086259295140323|\n",
      "| -4.682649077263664|\n",
      "| -4.872696719793019|\n",
      "|-2.8496170326188874|\n",
      "| -4.393711343103702|\n",
      "| -5.026771649986441|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(spark_df.n - 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895e7d2",
   "metadata": {},
   "source": [
    "iv. Multiply the `n` column by 2. View the results along with the original numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a58e8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|             (n * 2)|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -1.424781324101176|\n",
      "|   0.753766378659703|   1.507532757319406|\n",
      "|-0.04450307833805...|-0.08900615667610691|\n",
      "| 0.45181233874578974|  0.9036246774915795|\n",
      "|  1.3451017084510097|  2.6902034169020195|\n",
      "|  0.5323378882945463|  1.0646757765890926|\n",
      "|  1.3501878997225267|  2.7003757994450535|\n",
      "|  0.8612113741693206|  1.7224227483386412|\n",
      "|  1.4786857374358966|   2.957371474871793|\n",
      "| -1.0453771305385342| -2.0907542610770684|\n",
      "| -0.7889890249515489| -1.5779780499030978|\n",
      "|  -1.261605945319069|  -2.523211890638138|\n",
      "|  0.5628467852810314|  1.1256935705620628|\n",
      "|-0.24332625188556253|-0.48665250377112507|\n",
      "|  0.9137407048596775|   1.827481409719355|\n",
      "| 0.31735092273633597|  0.6347018454726719|\n",
      "| 0.12730328020698067| 0.25460656041396135|\n",
      "|  2.1503829673811126|   4.300765934762225|\n",
      "|  0.6062886568962988|  1.2125773137925977|\n",
      "|-0.02677164998644...|-0.05354329997288145|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(spark_df.n, spark_df.n * 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff1c9f",
   "metadata": {},
   "source": [
    "v. Add a new column named `n2` that is the `n` value multiplied by -1. Show the first 4 rows of your dataframe. You should see the original `n` value as well as `n2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37a1b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+\n",
      "|                   n|group|abool|                  n2|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974|\n",
      "+--------------------+-----+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.withColumn('n2', expr('n * -1'))\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a273fb",
   "metadata": {},
   "source": [
    "vi. Add a new column named `n3` that is the `n` value squared. Show the first 5 rows of your dataframe. You should see both `n`, `n2`, and `n3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec2353fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|                   n|group|abool|                  n2|                  n3|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "|  -0.712390662050588|    z|false|   0.712390662050588|   0.507500455376875|\n",
      "|   0.753766378659703|    x|false|  -0.753766378659703|  0.5681637535977627|\n",
      "|-0.04450307833805...|    z|false|0.044503078338053455|0.001980523981562...|\n",
      "| 0.45181233874578974|    y|false|-0.45181233874578974| 0.20413438944294027|\n",
      "+--------------------+-----+-----+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('n3', expr('n * n'))\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f535e4",
   "metadata": {},
   "source": [
    "vii. What happens when you run the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7deed9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(group + abool)'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group + df.abool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d00b7",
   "metadata": {},
   "source": [
    "**A**: A Column object is produced that represents the transformation of adding together the `group` and `abool` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa6499",
   "metadata": {},
   "source": [
    "viii. What happens when you run the code below? What is the difference between this and the previous code sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd353193",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(CAST(group AS DOUBLE) + abool)' due to data type mismatch: differing types in '(CAST(group AS DOUBLE) + abool)' (double and boolean).;\n'Project [unresolvedalias((cast(group#1 as double) + abool#2), Some(org.apache.spark.sql.Column$$Lambda$3234/0x00000008011f2840@4175a572))]\n+- Project [n#0, group#1, abool#2, n2#361, (n#0 * n#0) AS n3#387]\n   +- Project [n#0, group#1, abool#2, (n#0 * cast(-1 as double)) AS n2#361]\n      +- LogicalRDD [n#0, group#1, abool#2], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-392aef8ccead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(CAST(group AS DOUBLE) + abool)' due to data type mismatch: differing types in '(CAST(group AS DOUBLE) + abool)' (double and boolean).;\n'Project [unresolvedalias((cast(group#1 as double) + abool#2), Some(org.apache.spark.sql.Column$$Lambda$3234/0x00000008011f2840@4175a572))]\n+- Project [n#0, group#1, abool#2, n2#361, (n#0 * n#0) AS n3#387]\n   +- Project [n#0, group#1, abool#2, (n#0 * cast(-1 as double)) AS n2#361]\n      +- LogicalRDD [n#0, group#1, abool#2], false\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group + df.abool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803ce4e",
   "metadata": {},
   "source": [
    "An error is produced referencing the incompatible types. Unlike the previous code sample, this one is done within the context of a `.select`, so even though there are still no values produced (we haven't invoked an action yet), spark is aware that the types are incompatible.\n",
    "\n",
    "ix. Try adding various other columns together. What are the results of combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750108ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac86b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad40192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff77c51",
   "metadata": {},
   "source": [
    "#### 3. Type Casting\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9e9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dbb20cc",
   "metadata": {},
   "source": [
    "ii. Use `.printSchema()` to view the datatypes in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e22eabe",
   "metadata": {},
   "source": [
    "iii. Use `.dtypes` to view the datatypes in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe43beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a8621a",
   "metadata": {},
   "source": [
    "iv. What is the difference between the two code samples below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.abool.cast('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.abool.cast('int')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76990605",
   "metadata": {},
   "source": [
    "**A:** One is a creating a Column and one is using that same column in a `.select()` in order to view the results of the cast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5556951",
   "metadata": {},
   "source": [
    "v. Use `.select()` and `.cast()` to convert the abool column to an integer type. View the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a2263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35549949",
   "metadata": {},
   "source": [
    "vi. Convert the `group` column to a integer data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2e8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65e8f127",
   "metadata": {},
   "source": [
    "vii. Convert the `n` column to a integer data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ddf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0321303",
   "metadata": {},
   "source": [
    "viii. Convert the `abool` column to a string data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53353b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a7bf7e",
   "metadata": {},
   "source": [
    "#### 4. Built-in Functions\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91e64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3113e26b",
   "metadata": {},
   "source": [
    "ii. Import the necessary functions from `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd19845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, mean, lit, concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b9dfd",
   "metadata": {},
   "source": [
    "iii. Find the highest `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d891af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa6944b5",
   "metadata": {},
   "source": [
    "iv. Find the lowest `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b2176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abc61f6",
   "metadata": {},
   "source": [
    "v. Find the average `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea341e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba73026",
   "metadata": {},
   "source": [
    "vi. Use `concat()` to change the group column to say \"Group: x\" or \"Group: y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbaa87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea48b3c9",
   "metadata": {},
   "source": [
    "vii. Use `concat()` to combine the `n` and `group` columns to produce results that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ffa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "000c6e50",
   "metadata": {},
   "source": [
    "#### 5. When / Otherwise\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ecf982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a5729d",
   "metadata": {},
   "source": [
    "ii. Use `when()` and `.otherwise()` to create a column that contains the text \"It is true\" when abool is true and \"It is false\"\" when abool is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a0fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7150a4d",
   "metadata": {},
   "source": [
    "iii. Create a column that contains 0 if n is less than 0, otherwise, the original `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c2a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a59fcb",
   "metadata": {},
   "source": [
    "#### 6. Filter / Where\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfffa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bdd3450",
   "metadata": {},
   "source": [
    "ii. Use `.filter()` or `.where()` to select just the rows where the group is y and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9791666f",
   "metadata": {},
   "source": [
    "iii. Select just the columns where the `abool` column is false and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebdc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b79d8f8",
   "metadata": {},
   "source": [
    "iv. Find the columns where the group column is not y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30b8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388e87b9",
   "metadata": {},
   "source": [
    "v. Find the columns where `n` is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2aa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abae76e",
   "metadata": {},
   "source": [
    "vi. Find the columns where `abool` is true and the group column is z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed5a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d539fa2",
   "metadata": {},
   "source": [
    "vii. Find the columns where `abool` is true or the `group` column is z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf1fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e28969",
   "metadata": {},
   "source": [
    "viii. Find the columns where `abool` is false and `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecb663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2a1457a",
   "metadata": {},
   "source": [
    "ix. Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab71e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2667721b",
   "metadata": {},
   "source": [
    "#### 7. Sorting\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa548d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032a4315",
   "metadata": {},
   "source": [
    "ii. Sort by the `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4d70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea9e4536",
   "metadata": {},
   "source": [
    "iii. Sort by the `group` value, both ascending and descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201ab57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5269b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05ef1e95",
   "metadata": {},
   "source": [
    "iv. Sort by the `group` value first, then, within each group, sort by `n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a8b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "891ad917",
   "metadata": {},
   "source": [
    "v. Sort by `abool`, `group`, and `n`. Does it matter in what order you specify the columns when sorting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d8736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24722352",
   "metadata": {},
   "source": [
    "**A:** It does matter as it determines in what order they will be sorted. When the values for the first specified column are the same, the next specified column will determine sort order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f34487",
   "metadata": {},
   "source": [
    "#### 8. Spark SQL\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9bfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afedc2cc",
   "metadata": {},
   "source": [
    "ii. Turn your dataframe into a table that can be queried with spark SQL. Name the table `my_df`. Answer the rest of the questions in this section with a spark sql query (`spark.sql`) against `my_df`. After each step, view the first 7 records from the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4441178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ae36cd3",
   "metadata": {},
   "source": [
    "iii. Write a query that shows all of the columns from your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1c3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "686512e4",
   "metadata": {},
   "source": [
    "iv. Write a query that shows just the `n` and `abool` columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ebbd2e2",
   "metadata": {},
   "source": [
    "v. Write a query that shows just the `n` and `group` columns. Rename the `group` column to `g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b679e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64198546",
   "metadata": {},
   "source": [
    "vi. Write a query that selects `n`, and creates two new columns: `n2`, the original `n` values halved, and `n3`: the original `n` values minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f48165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e881c247",
   "metadata": {},
   "source": [
    "vii. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fa8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6875af",
   "metadata": {},
   "source": [
    "#### 9. Aggregating\n",
    "\n",
    "i. Use the starter code above to re-create a spark dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a169e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7effd53f",
   "metadata": {},
   "source": [
    "ii. What is the average `n` value for each group in the `group` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfdd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d27a4b6",
   "metadata": {},
   "source": [
    "iii. What is the maximum `n` value for each group in the `group` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac476d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923acac0",
   "metadata": {},
   "source": [
    "iv. What is the minimum `n` value by `abool`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866b4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ff801d",
   "metadata": {},
   "source": [
    "v. What is the average `n` value for each unique combination of the `group` and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c13a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
